{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert_ImageCLEF_SPLL_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mainguyenanhvu/Clinic_Managerment_Application/blob/master/Convert_ImageCLEF_SPLL_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmkqzuhG42c",
        "colab_type": "code",
        "outputId": "c8fb173f-bc8d-4772-dd9e-c1eba7f5b126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!rm -r /content/spll\n",
        "#!cd /content\n",
        "!rm -r spll/data/preprocessed\n",
        "!rm -r spll/data/merged\n",
        "!mkdir spll/data/merged\n",
        "!mkdir spll/data/preprocessed\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdd8KuccIl8J",
        "colab_type": "text"
      },
      "source": [
        "#DOWNLOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Mzed08pL03",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "1799317d-689f-40b2-ae8e-9959404243eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#@title Default title text\n",
        "# Download data\n",
        "\n",
        "!mkdir spll\n",
        "!mkdir spll/data\n",
        "!mkdir spll/data/unzip\n",
        "!mkdir spll/data/converted\n",
        "!mkdir spll/data/merged\n",
        "!mkdir spll/data/preprocessed\n",
        "!mkdir spll/data/database\n",
        "%cd spll/data\n",
        "!wget https://aicrowd-production.s3.eu-central-1.amazonaws.com/task_dataset_files/clef_task_26/09141252-f82d-4910-aac4-1c9e229cc324_ImageCLEF_SPLL_Data.zip\n",
        "!unzip 09141252-f82d-4910-aac4-1c9e229cc324_ImageCLEF_SPLL_Data.zip -d ./unzip > unzip_log.txt\n",
        "%cd ../.."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spll/data\n",
            "--2020-04-22 23:39:45--  https://aicrowd-production.s3.eu-central-1.amazonaws.com/task_dataset_files/clef_task_26/09141252-f82d-4910-aac4-1c9e229cc324_ImageCLEF_SPLL_Data.zip\n",
            "Resolving aicrowd-production.s3.eu-central-1.amazonaws.com (aicrowd-production.s3.eu-central-1.amazonaws.com)... 52.219.47.37\n",
            "Connecting to aicrowd-production.s3.eu-central-1.amazonaws.com (aicrowd-production.s3.eu-central-1.amazonaws.com)|52.219.47.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1815394273 (1.7G) [binary/octet-stream]\n",
            "Saving to: ‘09141252-f82d-4910-aac4-1c9e229cc324_ImageCLEF_SPLL_Data.zip’\n",
            "\n",
            "09141252-f82d-4910- 100%[===================>]   1.69G  30.0MB/s    in 59s     \n",
            "\n",
            "2020-04-22 23:40:45 (29.2 MB/s) - ‘09141252-f82d-4910-aac4-1c9e229cc324_ImageCLEF_SPLL_Data.zip’ saved [1815394273/1815394273]\n",
            "\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjJ6jA8GIqXB",
        "colab_type": "text"
      },
      "source": [
        "#IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgwMlB7TsKbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import json as js\n",
        "from os import path, listdir, mkdir\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import functools\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urSNe_T4sw5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define constants\n",
        "\n",
        "DATA_DIR = './spll/data/unzip'\n",
        "CONVERTED_DIR = './spll/data/converted'\n",
        "MERGED_DIR = './spll/data/merged'\n",
        "PRE_PROCESSED_DIR = './spll/data/preprocessed'\n",
        "DATABASE = './spll/data/database'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjUSr369Ivd2",
        "colab_type": "text"
      },
      "source": [
        "#CONVERT JSON TO CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI52UCFKI0cW",
        "colab_type": "text"
      },
      "source": [
        "##Calories file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubf8OuX1rQj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to calories files (calories, distance, steps, active minutes)\n",
        "\n",
        "def convert_calories_format_data_file(infile, outfile):\n",
        "  with open(infile) as source:\n",
        "    calories = js.load(source)\n",
        "    calories_df = pd.DataFrame(calories)\n",
        "    calories_df.to_csv(outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypyxda3uI8yo",
        "colab_type": "text"
      },
      "source": [
        "##Heart rate file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTOlJhPxem0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to heart rate files\n",
        "\n",
        "def convert_heart_rate_format_data_file(infile, outfile):\n",
        "  with open(infile) as source:\n",
        "    heart_rate = js.load(source)\n",
        "    for value in heart_rate:\n",
        "      value['bpm'] = value['value']['bpm']\n",
        "      value['confidence'] = value['value']['confidence']\n",
        "      value.pop('value', None)\n",
        "    heart_rate_df = pd.DataFrame(heart_rate)\n",
        "    heart_rate_df.to_csv(outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8sUzkICJBbH",
        "colab_type": "text"
      },
      "source": [
        "##Heart rate zone file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ha_RIgfiNCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to heart rate zones files\n",
        "\n",
        "def convert_heart_rate_zones_format_data_file(infile, outfile):\n",
        "  with open(infile) as source:\n",
        "    heart_rate_zones = js.load(source)\n",
        "    for value in heart_rate_zones:\n",
        "      value['IN_DEFAULT_ZONE_1'] = value['value']['valuesInZones']['IN_DEFAULT_ZONE_1']\n",
        "      value['IN_DEFAULT_ZONE_2'] = value['value']['valuesInZones']['IN_DEFAULT_ZONE_2']\n",
        "      value['IN_DEFAULT_ZONE_3'] = value['value']['valuesInZones']['IN_DEFAULT_ZONE_3']\n",
        "      value['BELOW_DEFAULT_ZONE_1'] = value['value']['valuesInZones']['BELOW_DEFAULT_ZONE_1']\n",
        "      value.pop('value', None)\n",
        "    heart_rate_zones_df = pd.DataFrame(heart_rate_zones)\n",
        "    heart_rate_zones_df.to_csv(outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdy5jLlVJLI_",
        "colab_type": "text"
      },
      "source": [
        "##Sleep file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaDZviXBlneW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to sleep files\n",
        "\n",
        "def convert_sleep_format_data_file(infile, sleep_outfile, level_outfile, data_outfile):\n",
        "  with open(infile) as source:\n",
        "    sleep = js.load(source)\n",
        "\n",
        "    sleep_level_summaries = []\n",
        "    sleep_level_datas = []\n",
        "    for value in sleep:\n",
        "      sleep_level_summaries.append(value['levels']['summary'])\n",
        "      sleep_level_datas.append(value['levels']['data'])\n",
        "      value.pop('levels', None)\n",
        "      value.pop('logId', None)\n",
        "    \n",
        "    for i in range(len(sleep)):\n",
        "      value = sleep_level_summaries[i]\n",
        "      value['dateOfSleep'] = sleep[i]['dateOfSleep']\n",
        "\n",
        "      try:\n",
        "        value['deep_count'] = value['deep']['count']\n",
        "        value['deep_minutes'] = value['deep']['minutes']\n",
        "        value['deep_thirty_day_avg_minutes'] = value['deep']['thirtyDayAvgMinutes']\n",
        "\n",
        "        value['wake_count'] = value['wake']['count']\n",
        "        value['wake_minutes'] = value['wake']['minutes']\n",
        "        value['wake_thirty_day_avg_minutes'] = value['wake']['thirtyDayAvgMinutes']\n",
        "\n",
        "        value['light_count'] = value['light']['count']\n",
        "        value['light_minutes'] = value['light']['minutes']\n",
        "        value['light_thirty_day_avg_minutes'] = value['light']['thirtyDayAvgMinutes']\n",
        "\n",
        "        value['rem_count'] = value['rem']['count']\n",
        "        value['rem_minutes'] = value['rem']['minutes']\n",
        "        value['rem_thirty_day_avg_minutes'] = value['rem']['thirtyDayAvgMinutes']\n",
        "\n",
        "        value.pop('deep', None)\n",
        "        value.pop('light', None)\n",
        "        value.pop('rem', None)\n",
        "        value.pop('wake', None)\n",
        "      except KeyError:\n",
        "        value['deep_count'] = value['deep_minutes'] = value['deep_thirty_day_avg_minutes'] = None\n",
        "\n",
        "        value['wake_count'] = value['wake_minutes'] = value['wake_thirty_day_avg_minutes'] = None\n",
        "\n",
        "        value['light_count'] = value['light_minutes'] = value['light_thirty_day_avg_minutes'] = None\n",
        "\n",
        "        value['rem_count'] = value['rem_minutes'] = value['rem_thirty_day_avg_minutes'] = None\n",
        "\n",
        "        value.pop('restless', None)\n",
        "        value.pop('awake', None)\n",
        "        value.pop('asleep', None)\n",
        "\n",
        "      datas = sleep_level_datas[i]\n",
        "      for data in datas:\n",
        "        data['dateOfSleep'] = sleep[i]['dateOfSleep']\n",
        "\n",
        "    sleep_level_datas = functools.reduce(lambda x, y: x+y, sleep_level_datas)\n",
        "\n",
        "    sleep_level_summaries_df = pd.DataFrame(sleep_level_summaries)\n",
        "    sleep_level_datas_df = pd.DataFrame(sleep_level_datas)\n",
        "    sleep_df = pd.DataFrame(sleep)\n",
        "\n",
        "    sleep_df.to_csv(sleep_outfile, header=True, index=False)\n",
        "    sleep_level_summaries_df.to_csv(level_outfile, header=True, index=False)\n",
        "    sleep_level_datas_df.to_csv(data_outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEv5TxebJS45",
        "colab_type": "text"
      },
      "source": [
        "##Custom heart rate file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbjB8VPnF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to custom heart rate files\n",
        "\n",
        "def convert_custom_heart_rate_format_data_file(infile, outfile):\n",
        "  with open(infile) as source:\n",
        "    custom_heart_rate = js.load(source)\n",
        "    for value in custom_heart_rate:\n",
        "      try:\n",
        "        value['enabled'] = value['value']['enabled']\n",
        "        value['maxHeartRate'] = value['value']['maxHeartRate']\n",
        "      except KeyError:\n",
        "        try:\n",
        "          value['minThreshold'] = value['value']['minThreshold']\n",
        "          value['maxThreshold'] = value['value']['maxThreshold']\n",
        "        except KeyError:\n",
        "          value['error'] = value['value']['error']\n",
        "          value['resting heart rate'] = value['value']['value']\n",
        "      value.pop('value', None)\n",
        "    custom_heart_rate_df = pd.DataFrame(custom_heart_rate)\n",
        "    custom_heart_rate_df.to_csv(outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfv2F1ljJWTZ",
        "colab_type": "text"
      },
      "source": [
        "##Exercise file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9nXZouG_U9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert files have the format which sames to exercise files\n",
        "\n",
        "def convert_exercise_format_data_file(infile, outfile):\n",
        "  with open(infile) as source:\n",
        "    exercise = js.load(source)\n",
        "    \n",
        "    for value in exercise:\n",
        "      value['sedentary_active_minutes'] = [x for x in value['activityLevel'] if x['name']=='sedentary'][0]['minutes']\n",
        "      value['lightly_active_minutes'] = [x for x in value['activityLevel'] if x['name']=='lightly'][0]['minutes']\n",
        "      value['fairly_active_minutes'] = [x for x in value['activityLevel'] if x['name']=='fairly'][0]['minutes']\n",
        "      value['very_active_minutes'] = [x for x in value['activityLevel'] if x['name']=='very'][0]['minutes']\n",
        "\n",
        "      try:\n",
        "        value['time_in_fat_burn_heart_rate_zone'] = [x for x in value['heartRateZones'] if x['name']=='Fat Burn'][0]['minutes']\n",
        "        value['time_in_out_of_range_heart_rate_zone'] = [x for x in value['heartRateZones'] if x['name']=='Out of Range'][0]['minutes']\n",
        "        value['time_in_peak_heart_rate_zone'] = [x for x in value['heartRateZones'] if x['name']=='Peak'][0]['minutes']\n",
        "        value['time_in_cardio_heart_rate_zone'] = [x for x in value['heartRateZones'] if x['name']=='Cardio'][0]['minutes']\n",
        "      except KeyError:\n",
        "        value['time_in_fat_burn_heart_rate_zone'] = None\n",
        "        value['time_in_out_of_range_heart_rate_zone'] = None\n",
        "        value['time_in_peak_heart_rate_zone'] = None\n",
        "        value['time_in_cardio_heart_rate_zone'] = None\n",
        "      \n",
        "      try:\n",
        "        value['vo2Max'] = value['vo2Max']['vo2Max']\n",
        "      except KeyError:\n",
        "        value['vo2Max'] = None\n",
        "\n",
        "      value.pop('logId', None)\n",
        "      value.pop('source', None)\n",
        "      value.pop('logType', None)\n",
        "      value.pop('manualValuesSpecified', None)\n",
        "      value.pop('tcxLink', None)\n",
        "      value.pop('shouldFetchDetails', None)\n",
        "      value.pop('hasGps', None)\n",
        "      value.pop('activityTypeId', None)\n",
        "      value.pop('activityLevel', None)\n",
        "      value.pop('heartRateZones', None)\n",
        "\n",
        "    exercise_df = pd.DataFrame(exercise)\n",
        "    exercise_df.to_csv(outfile, header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02N5w5m2Jcjp",
        "colab_type": "text"
      },
      "source": [
        "##Convert all file in fitbit folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHXQM8PTuwRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_fitbit_folder():\n",
        "  for d in listdir(DATA_DIR):\n",
        "    d_path = path.join(DATA_DIR, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "\n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "      mkdir(path.join(converted_d_path,'fitbit'))\n",
        "    \n",
        "    d_path = path.join(d_path, 'fitbit')\n",
        "    converted_d_path = path.join(converted_d_path, 'fitbit')\n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      if re.search('^(calories|distance|steps|very_active_minutes|moderately_active_minutes|lightly_active_minutes|sedentary_minutes).*(json)$', f):\n",
        "        convert_calories_format_data_file(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'))\n",
        "      \n",
        "      if re.search('^(heart_rate).*(json)$', f):\n",
        "        convert_heart_rate_format_data_file(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'))\n",
        "\n",
        "      if re.search('^(time_in_heart_rate_zones).*(json)$', f):\n",
        "        convert_heart_rate_zones_format_data_file(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'))\n",
        "\n",
        "      if re.search('^(custom|resting_heart_rate).*(json)$', f):\n",
        "        convert_custom_heart_rate_format_data_file(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'))\n",
        "\n",
        "      if re.search('^(sleep).*(json)$', f):\n",
        "        sleep_outfile = path.join(converted_d_path, f.split('.')[0] + '.csv')\n",
        "        level_outfile = path.join(converted_d_path, 'level_' + f.split('.')[0] + '.csv')\n",
        "        data_outfile = path.join(converted_d_path, 'data_' + f.split('.')[0] + '.csv')\n",
        "        convert_sleep_format_data_file(f_path, sleep_outfile, level_outfile, data_outfile)\n",
        "      \n",
        "      if re.search('^(exercise).*(json)$', f):\n",
        "        convert_exercise_format_data_file(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq7sWJPZJiyM",
        "colab_type": "text"
      },
      "source": [
        "#Merge files in fitbit folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXwdKT6OxWKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge files in fitbit folder\n",
        "\n",
        "def merge_fitbit_folder():\n",
        "  for d in listdir(CONVERTED_DIR):\n",
        "    d_path = path.join(CONVERTED_DIR, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "\n",
        "    merged_d_path= path.join(MERGED_DIR, d)\n",
        "    if not(path.exists(merged_d_path)):\n",
        "      mkdir(merged_d_path)\n",
        "    \n",
        "    calories = []\n",
        "    distances = []\n",
        "    steps = []\n",
        "    heart_rates = []\n",
        "    very_active_minutes = []\n",
        "    moderately_active_minutes = []\n",
        "    lightly_active_minutes = []\n",
        "    sedentary_minutes = []\n",
        "    time_in_zones = []\n",
        "    sleeps = []\n",
        "    exercises = []\n",
        "    level_sleeps = []\n",
        "    data_sleeps = []\n",
        "\n",
        "    # calories_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/calories-2019-10-08.csv')).columns)\n",
        "    # distances_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/distance-2019-10-08.csv')).columns)\n",
        "    # steps_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/steps-2019-10-08.csv')).columns)\n",
        "    # heart_rates_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/heart_rate-2019-10-10.csv')).columns)\n",
        "    # very_active_minutes_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/very_active_minutes-2019-10-08.csv')).columns)\n",
        "    # moderately_active_minutes_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/moderately_active_minutes-2019-10-08.csv')).columns)\n",
        "    # lightly_active_minutes_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/lightly_active_minutes-2019-10-08.csv')).columns)\n",
        "    # sedentary_minutes_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/sedentary_minutes-2019-10-08.csv')).columns)\n",
        "    # time_in_zones_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/time_in_heart_rate_zones-2019-10-10.csv')).columns)\n",
        "    # sleeps_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/sleep-2019-10-08.csv')).columns)\n",
        "    # exercises_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/exercise-0.csv')).columns)\n",
        "    # level_sleeps_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/level_sleep-2019-10-08.csv')).columns)\n",
        "    # data_sleeps_header = list(pd.read_csv(path.join(CONVERTED_DIR, 'p01/fitbit/data_sleep-2019-10-08.csv')).columns)\n",
        "\n",
        "    d_path = path.join(d_path, 'fitbit')\n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "\n",
        "      df = pd.read_csv(f_path)\n",
        "      \n",
        "      if re.search('^(calories).*(csv)$', f):\n",
        "        calories.append(df)\n",
        "        \n",
        "      if re.search('^(distance).*(csv)$', f):\n",
        "        distances.append(df)\n",
        "\n",
        "      if re.search('^(steps).*(csv)$', f):\n",
        "        steps.append(df)\n",
        "\n",
        "      if re.search('^(very_active_minutes).*(csv)$', f):\n",
        "        very_active_minutes.append(df)\n",
        "\n",
        "      if re.search('^(moderately_active_minutes).*(csv)$', f):\n",
        "        moderately_active_minutes.append(df)\n",
        "\n",
        "      if re.search('^(lightly_active_minutes).*(csv)$', f):\n",
        "        lightly_active_minutes.append(df)\n",
        "\n",
        "      if re.search('^(sedentary_minutes).*(csv)$', f):\n",
        "        sedentary_minutes.append(df)\n",
        "      \n",
        "      if re.search('^(heart_rate).*(csv)$', f):\n",
        "        heart_rates.append(df)\n",
        "\n",
        "      if re.search('^(time_in_heart_rate_zones).*(csv)$', f):\n",
        "        time_in_zones.append(df)\n",
        "\n",
        "      if re.search('^(sleep).*(csv)$', f):\n",
        "        sleeps.append(df)\n",
        "\n",
        "      if re.search('^(level_sleep).*(csv)$', f):\n",
        "        level_sleeps.append(df)\n",
        "\n",
        "      if re.search('^(data_sleep).*(csv)$', f):\n",
        "        data_sleeps.append(df)\n",
        "      \n",
        "      if re.search('^(exercise).*(csv)$', f):\n",
        "        exercises.append(df)\n",
        "\n",
        "    if len(sleeps) != 0:\n",
        "      sleeps_df = pd.concat(sleeps)\n",
        "      sleeps_df.to_csv(path.join(merged_d_path, 'sleep.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'sleep.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(sleeps_header))\n",
        "\n",
        "    if len(level_sleeps) != 0:\n",
        "      level_sleeps_df = pd.concat(level_sleeps)\n",
        "      level_sleeps_df.to_csv(path.join(merged_d_path, 'level_sleep.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'level_sleep.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(level_sleeps_header))\n",
        "\n",
        "    if len(sleeps) != 0:\n",
        "      data_sleeps_df = pd.concat(data_sleeps)\n",
        "      data_sleeps_df.to_csv(path.join(merged_d_path, 'data_sleep.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'data_sleep.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(data_sleeps_header))\n",
        "\n",
        "    if len(exercises) != 0:\n",
        "      exercises_df = pd.concat(exercises)\n",
        "      exercises_df.to_csv(path.join(merged_d_path, 'exercise.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'exercise.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(exercises_header))\n",
        "\n",
        "    if len(time_in_zones) != 0:\n",
        "      time_in_zones_df = pd.concat(time_in_zones)\n",
        "      time_in_zones_df.to_csv(path.join(merged_d_path, 'time_in_heart_rate_zones.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'time_in_heart_rate_zones.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(time_in_heart_rate_zones_header))\n",
        "\n",
        "    if len(heart_rates) != 0:\n",
        "      heart_rates_df = pd.concat(heart_rates)\n",
        "      heart_rates_df.to_csv(path.join(merged_d_path, 'heart_rate.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'heart_rate.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(heart_rates_header))\n",
        "\n",
        "    if len(sedentary_minutes) != 0:\n",
        "      sedentary_minutes_df = pd.concat(sedentary_minutes)\n",
        "      sedentary_minutes_df.to_csv(path.join(merged_d_path, 'sedentary_minutes.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'sedentary_minutes.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(sedentary_minutes_header))\n",
        "\n",
        "    if len(lightly_active_minutes) != 0:\n",
        "      lightly_active_minutes_df = pd.concat(lightly_active_minutes)\n",
        "      lightly_active_minutes_df.to_csv(path.join(merged_d_path, 'lightly_active_minutes.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'lightly_active_minutes.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(lightly_active_minutes_header))\n",
        "\n",
        "    if len(moderately_active_minutes) != 0:\n",
        "      moderately_active_minutes_df = pd.concat(moderately_active_minutes)\n",
        "      moderately_active_minutes_df.to_csv(path.join(merged_d_path, 'moderately_active_minutes.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'moderately_active_minutes.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(moderately_active_minutes_header))\n",
        "\n",
        "    if len(very_active_minutes) != 0:\n",
        "      very_active_minutes_df = pd.concat(very_active_minutes)\n",
        "      very_active_minutes_df.to_csv(path.join(merged_d_path, 'very_active_minutes.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'very_active_minutes.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(very_active_minutes_header))\n",
        "\n",
        "    if len(steps) != 0:\n",
        "      steps_df = pd.concat(steps)\n",
        "      steps_df.to_csv(path.join(merged_d_path, 'steps.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'steps.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(steps_header))\n",
        "\n",
        "    if len(distances) != 0:\n",
        "      distances_df = pd.concat(distances)\n",
        "      distances_df.to_csv(path.join(merged_d_path, 'distance.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'distances.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(distances_header))\n",
        "\n",
        "    if len(calories) != 0:\n",
        "      calories_df = pd.concat(calories)\n",
        "      calories_df.to_csv(path.join(merged_d_path, 'calories.csv'), header=True, index=False)\n",
        "    # else:\n",
        "    #   with open(path.join(merged_d_path, 'calories.csv'), 'w') as f:\n",
        "    #     f.write(', '.join(calories_header))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoqnOxvD-EIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge multi filetypes into one\n",
        "\n",
        "def merge_multi_file_type_of_fitbit_folder(in_dir, out_dir):\n",
        "  for d in listdir(in_dir):\n",
        "    d_path = path.join(in_dir, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "    \n",
        "    calories_df = None\n",
        "    distance_df = None\n",
        "    steps_df = None\n",
        "    heart_rate_df = None\n",
        "    lightly_active_minutes_df = None\n",
        "    very_active_minutes = None\n",
        "    sedentary_minutes = None\n",
        "    moderately_active_minutes = None\n",
        "\n",
        "    \n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "\n",
        "      if re.search('^(calories).*(csv)$', f):\n",
        "        calories_df = pd.read_csv(f_path)\n",
        "        calories_df.rename(columns={'value': 'calories'}, inplace=True)\n",
        "      \n",
        "      if re.search('^(heart_rate).*(csv)$', f):\n",
        "        heart_rate_df = pd.read_csv(f_path)\n",
        "        heart_rate_df.rename(columns={'bpm': 'heart_rate', 'confidence': 'heart_rate_confidence'}, inplace=True)\n",
        "\n",
        "      if re.search('^(distance).*(csv)$', f):\n",
        "        distance_df = pd.read_csv(f_path)\n",
        "        distance_df.rename(columns={'value': 'distance'}, inplace=True)\n",
        "      \n",
        "      if re.search('^(steps).*(csv)$', f):\n",
        "        steps_df = pd.read_csv(f_path)\n",
        "        steps_df.rename(columns={'value': 'steps'}, inplace=True)\n",
        "\n",
        "      if re.search('^(very_active_minutes).*(csv)$', f):\n",
        "        very_active_minutes = pd.read_csv(f_path)\n",
        "        very_active_minutes.rename(columns={'value': 'very_active'}, inplace=True)\n",
        "\n",
        "      if re.search('^(moderately_active_minutes).*(csv)$', f):\n",
        "        moderately_active_minutes_df = pd.read_csv(f_path)\n",
        "        moderately_active_minutes_df.rename(columns={'value': 'moderately_active'}, inplace=True)\n",
        "\n",
        "      if re.search('^(lightly_active_minutes).*(csv)$', f):\n",
        "        lightly_active_minutes_df = pd.read_csv(f_path)\n",
        "        lightly_active_minutes_df.rename(columns={'value': 'lightly_active'}, inplace=True)\n",
        "\n",
        "      if re.search('^(sedentary_minutes).*(csv)$', f):\n",
        "        sedentary_minutes_df = pd.read_csv(f_path)\n",
        "        sedentary_minutes_df.rename(columns={'value': 'sedentary'}, inplace=True)\n",
        "\n",
        "    calories_df.drop_duplicates(subset='dateTime', keep='first', inplace=True)\n",
        "    distance_df.drop_duplicates(subset='dateTime', keep='first', inplace=True)\n",
        "    steps_df.drop_duplicates(subset='dateTime', keep='first', inplace=True)\n",
        "    heart_rate_df.drop_duplicates(subset='dateTime', keep='first', inplace=True)\n",
        "\n",
        "    out_path = path.join(out_dir, d)\n",
        "    if not(path.exists(out_path)):\n",
        "      mkdir(out_path)\n",
        "\n",
        "    total_cal_dis_ste_hea_df = pd.concat([calories_df.set_index('dateTime'), distance_df.set_index('dateTime'), steps_df.set_index('dateTime'), heart_rate_df.set_index('dateTime')], axis=1)\n",
        "    total_active_minutes_df = pd.concat([very_active_minutes.set_index('dateTime'), moderately_active_minutes_df.set_index('dateTime'), lightly_active_minutes_df.set_index('dateTime'), sedentary_minutes_df.set_index('dateTime')], axis=1)\n",
        "\n",
        "    total_cal_dis_ste_hea_df.index.set_names('dateTime', inplace=True)\n",
        "    total_active_minutes_df.index.set_names('dateTime', inplace=True)\n",
        "    total_cal_dis_ste_hea_df.reset_index(inplace=True)\n",
        "    total_active_minutes_df.reset_index(inplace=True)\n",
        "\n",
        "    total_cal_dis_ste_hea_df.to_csv(path.join(out_path, 'total_cal_dis_ste_hea.csv'), header=True, index=False)\n",
        "    total_active_minutes_df.to_csv(path.join(out_path, 'total_active_minutes.csv'), header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNqKERtGJvnT",
        "colab_type": "text"
      },
      "source": [
        "#CONVER FILE IN PMSYS FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZtFelb6A5Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_injury(infile,outfile,filename):\n",
        "  print(outfile)\n",
        "  filetxt = str(infile).replace(filename,'info_'+filename.split('.')[0]+'.txt')\n",
        "  fi = open(filetxt,'r')\n",
        "  columns = fi.readline().replace(' ','').split(';')\n",
        "  fi.close()\n",
        "\n",
        "  fo = open(outfile,'w')\n",
        "  fo.write('{},{}\\n'.format(columns[0],columns[1]))\n",
        "  reader = csv.reader(open(infile,'r'))\n",
        "  for row in reader:\n",
        "    temp = str(row).replace(' ','').replace('[','').replace(']','').replace(\"'\",\"\").replace('\"','').replace(',','|')\n",
        "    #print(temp)\n",
        "    arow = temp.split(';')\n",
        "    fo.write('{},{}\\n'.format(arow[0],arow[1]))\n",
        "  fo.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV7yrDlEg2_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_srpe(infile,outfile,filename):\n",
        "  print(outfile)\n",
        "  filetxt = str(infile).replace(filename,'info_'+filename.split('.')[0]+'.txt')\n",
        "  fi = open(filetxt,'r')\n",
        "  columns = fi.readline().replace(' ','').split(';')\n",
        "  fi.close()\n",
        "\n",
        "  fo = open(outfile,'w')\n",
        "  fo.write('{},{},{},{}\\n'.format(columns[0],columns[1],columns[2],columns[3]))\n",
        "  reader = csv.reader(open(infile,'r'))\n",
        "  for row in reader:\n",
        "    temp = str(row).replace(' ','').replace('[','').replace(']','').replace(\"'\",\"\").replace('\"','').replace(',','|')\n",
        "    #print(temp)\n",
        "    arow = temp.split(';')\n",
        "    fo.write('{},{},{},{}\\n'.format(arow[0],arow[1],arow[3],arow[3]))\n",
        "  fo.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEipyi9Og5Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_wellness(infile,outfile,filename):\n",
        "  print(outfile)\n",
        "  filetxt = str(infile).replace(filename,'info_'+filename.split('.')[0]+'.txt')\n",
        "  fi = open(filetxt,'r')\n",
        "  columns = fi.readline().replace(' ','').split(';')\n",
        "  fi.close()\n",
        "\n",
        "  fo = open(outfile,'w')\n",
        "  fo.write('{},{},{},{},{},{},{},{},{}\\n'.format(columns[0],columns[1],columns[2],columns[3],columns[4],columns[5],columns[6],columns[7],columns[8]))\n",
        "  reader = csv.reader(open(infile,'r'))\n",
        "  for row in reader:\n",
        "    temp = str(row).replace(' ','').replace('[','').replace(']','').replace(\"'\",\"\").replace('\"','').replace(',','|')\n",
        "    #print(temp)\n",
        "    arow = temp.split(';')\n",
        "    fo.write('{},{},{},{},{},{},{},{},{}\\n'.format(arow[0],arow[1],arow[3],arow[3],arow[4],arow[5],arow[6],arow[7],arow[8]))\n",
        "  fo.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3g_VtTkg62X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_pmsys_folder():\n",
        "  #convert pmsys/[file]\n",
        "  for d in listdir(DATA_DIR):\n",
        "    d_path = path.join(DATA_DIR, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "\n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "      mkdir(path.join(converted_d_path,'pmsys'))\n",
        "    \n",
        "    d_path = path.join(d_path, 'pmsys')\n",
        "    converted_d_path = path.join(converted_d_path, 'pmsys')\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      #if path.exists(path.join(converted_d_path, f.split('.')[0] + '.csv')):\n",
        "        #continue\n",
        "      \n",
        "      #print(str(f).split('.'))\n",
        "      if str(f) == 'injury.csv':\n",
        "        #print(f_path)\n",
        "        convert_injury(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'),str(f))\n",
        "      if str(f) == 'srpe.csv':\n",
        "        convert_srpe(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'),str(f))\n",
        "      if str(f) == 'wellness.csv':\n",
        "        convert_srpe(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'),str(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XRDmz68J4Qo",
        "colab_type": "text"
      },
      "source": [
        "#CONVERT FILE IN GOOGLEDOCS FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLzvLFog8lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_reporting(infile,outfile,filename):\n",
        "  columns = \"\"\n",
        "  filetxt = str(infile).replace(filename,'info_'+filename.split('.')[0]+'.txt')\n",
        "  fi = open(filetxt,'r')\n",
        "  for line in fi.readlines():\n",
        "    columns += str(line)\n",
        "  fi.close()\n",
        "  columns = columns.replace('\\n','').replace(' ','').replace('p,','p;').replace('d,','d;').replace('?,','?;').replace('s,','s;').replace('\",','\";').replace(',','|').split(';')\n",
        "\n",
        "  df = pd.read_csv(infile)\n",
        "  df.columns = columns\n",
        "  for i in df.index:\n",
        "    stemp = str(df.at[i,'Eatenmeals']).replace(',','|')\n",
        "    df.at[i,'Eatenmeals'] = stemp\n",
        "  df.to_csv(outfile,header=True,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZSWahug-vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_googledocs_folder():\n",
        "  #convert googledocs/[file]\n",
        "  for d in listdir(DATA_DIR):\n",
        "    d_path = path.join(DATA_DIR, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "\n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "      mkdir(path.join(converted_d_path,'googledocs'))\n",
        "    \n",
        "    d_path = path.join(d_path, 'googledocs')\n",
        "    converted_d_path = path.join(converted_d_path, 'googledocs')\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      #if path.exists(path.join(converted_d_path, f.split('.')[0] + '.csv')):\n",
        "        #continue\n",
        "      \n",
        "      #print(str(f).split('.'))\n",
        "      if str(f) == 'reporting.csv':\n",
        "        print(f_path)\n",
        "        convert_reporting(f_path, path.join(converted_d_path, f.split('.')[0] + '.csv'),str(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIT6ATk7J-tN",
        "colab_type": "text"
      },
      "source": [
        "#COPY ALL CSV FILE TO MERGE FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJab0zx2GO1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_csv_merge_folder():\n",
        "  for d in listdir(DATA_DIR):\n",
        "    d_path = path.join(DATA_DIR, d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "\n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "      mkdir(path.join(converted_d_path,'fitbit'))\n",
        "    \n",
        "    d_path = path.join(d_path, 'fitbit')\n",
        "    converted_d_path = path.join(converted_d_path, 'fitbit')\n",
        "    if not(path.exists(converted_d_path)):\n",
        "      mkdir(converted_d_path)\n",
        "    \n",
        "    merged_d_path= path.join(MERGED_DIR, d)\n",
        "    if not(path.exists(merged_d_path)):\n",
        "      mkdir(merged_d_path)\n",
        "    \n",
        "    for f in listdir(d_path):\n",
        "      f_path = path.join(d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      #if path.exists(path.join(converted_d_path, f.split('.')[0] + '.csv')):\n",
        "        #continue\n",
        "      \n",
        "      #print(str(f).split('.'))\n",
        "      if str(f).split('.')[-1] == 'csv':\n",
        "        shutil.copyfile(f_path, path.join(merged_d_path, f.split('.')[0] + '.csv'))\n",
        "        print(f_path)\n",
        "      \n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    converted_d_path = path.join(converted_d_path, 'googledocs')\n",
        "    merged_d_path= path.join(MERGED_DIR, d)\n",
        "    if not(path.exists(merged_d_path)):\n",
        "      mkdir(merged_d_path)\n",
        "    for f in listdir(converted_d_path):\n",
        "      f_path = path.join(converted_d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      #if path.exists(path.join(converted_d_path, f.split('.')[0] + '.csv')):\n",
        "        #continue\n",
        "      \n",
        "      #print(str(f).split('.'))\n",
        "      if str(f).split('.')[-1] == 'csv':\n",
        "        shutil.copyfile(f_path, path.join(merged_d_path, f.split('.')[0] + '.csv'))\n",
        "        print(f_path)\n",
        "      \n",
        "    converted_d_path = path.join(CONVERTED_DIR, d)\n",
        "    converted_d_path = path.join(converted_d_path, 'pmsys')\n",
        "    merged_d_path= path.join(MERGED_DIR, d)\n",
        "    if not(path.exists(merged_d_path)):\n",
        "      mkdir(merged_d_path)\n",
        "    for f in listdir(converted_d_path):\n",
        "      f_path = path.join(converted_d_path, f)\n",
        "      if path.isdir(f):\n",
        "        continue\n",
        "      \n",
        "      #if path.exists(path.join(converted_d_path, f.split('.')[0] + '.csv')):\n",
        "        #continue\n",
        "      \n",
        "      #print(str(f).split('.'))\n",
        "      if str(f).split('.')[-1] == 'csv':\n",
        "        shutil.copyfile(f_path, path.join(merged_d_path, f.split('.')[0] + '.csv'))\n",
        "        print(f_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Qi1wJ7KE8l",
        "colab_type": "text"
      },
      "source": [
        "#PREPROCESS CSV FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJKJkr_-vnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_sleep(d_path,preprocessed_d_path):\n",
        "  df = pd.read_csv(path.join(d_path,'sleep.csv'))\n",
        "  \n",
        "  for co_name in df.columns:\n",
        "    if co_name.count('minutes') > 0:\n",
        "      df[co_name] = df[co_name]*60\n",
        "      df.rename(columns={co_name:co_name.replace('minutes','seconds')},inplace=True)\n",
        "  df.rename(columns={'dateOfSleep':'date'},inplace=True)\n",
        "  if df.empty:\n",
        "    return df\n",
        "  \n",
        "  df.drop('startTime',axis='columns',inplace=True)\n",
        "  df.endTime = df.endTime.str.replace('T', ' ').str.replace('.000','')\n",
        "  df.endTime = pd.to_datetime(df.endTime).values.astype(np.int64) // 10 ** 9\n",
        "  df.sort_values(by = 'endTime', ascending=False, inplace=True)\n",
        "  \n",
        "  #boolean = df['endTime'].duplicated().any()\n",
        "  boolean = False\n",
        "  if boolean:\n",
        "    print(d_path)\n",
        "    print(\"sleep duplicated\")\n",
        "    print(df[df.duplicated()])\n",
        "    #print(df[df.duplicated(['endTime'])])\n",
        "    print('---------------------------------2----------------------------------')\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvrfBYcJ--bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_sleepscore(d_path,preprocessed_d_path):\n",
        "  df_ss = pd.read_csv(path.join(d_path,'sleep_score.csv'))\n",
        "  df_ss.rename(columns={\"timestamp\": 'endTime'},inplace=True)\n",
        "  if df_ss.empty:\n",
        "    return df_ss\n",
        "  \n",
        "  df_ss.drop(df_ss[df_ss['overall_score'].isna()].index,inplace=True)\n",
        "  df_ss.drop('sleep_log_entry_id',axis = 'columns', inplace=True)\n",
        "  df_ss.endTime = df_ss.endTime.str.replace('T', ' ').str.replace('.000','')\n",
        "  df_ss.endTime = pd.to_datetime(df_ss.endTime).values.astype(np.int64) // 10 ** 9\n",
        "  for co_name in df_ss.columns:\n",
        "    if str(co_name).count('minutes')>0:\n",
        "      df_ss[co_name] = df_ss[co_name]*60\n",
        "  \n",
        "  #boolean = df_ss['endTime'].duplicated().any()\n",
        "  boolean = False\n",
        "  if boolean:\n",
        "    print(d_path)\n",
        "    print(\"sleep score duplicated\")\n",
        "    print(df_ss[df_ss.duplicated()])\n",
        "    #print(df_ss[df_ss.duplicated('endTime')])\n",
        "    print('---------------------------------1bbbbbbbbbbbb----------------------------------')\n",
        "  #print(df_ss)\n",
        "  return df_ss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgPS5V_5iYDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_levelsleep(d_path,preprocessed_d_path):\n",
        "  filepath = path.join(d_path,'level_sleep.csv')\n",
        "  if (not path.exists(filepath)):\n",
        "    return\n",
        "  df_ls = pd.read_csv(path.join(d_path,'level_sleep.csv'))\n",
        "  \n",
        "  for co_name in df_ls.columns:\n",
        "    if str(co_name).count('minutes')>0:\n",
        "      df_ls[co_name] = df_ls[co_name]*60\n",
        "      df_ls.rename(columns={co_name:co_name.replace('minutes','seconds')},inplace=True)\n",
        "  df_ls.rename(columns={\"dateOfSleep\": 'date'},inplace=True)\n",
        "  if df_ls.empty:\n",
        "    df_ls.to_csv(path.join(preprocessed_d_path,'level_sleep.csv'))\n",
        "    return df_ls\n",
        "  \n",
        "  #index_name = df_ls[df_ls['deep_count'].isna()].index\n",
        "  #df_ls.drop(index_name,inplace=True)\n",
        "  df_ls.sort_values(by = ['date','deep_thirty_day_avg_seconds'], ascending=False, inplace=True)\n",
        "  df_ls.drop_duplicates(inplace=True)\n",
        "  \n",
        "  boolean = df_ls['date'].duplicated().any()\n",
        "  boolean = False\n",
        "  if boolean:\n",
        "    print(d_path)\n",
        "    print(\"level sleep duplicated\")\n",
        "    print(df_ls[df_ls.duplicated()])\n",
        "    print(df_ls[df_ls.duplicated('date')])\n",
        "    print('---------------------------------1aaaaaaaaaaaaa----------------------------------')\n",
        "  df_ls = df_ls.loc[(df_ls['deep_thirty_day_avg_seconds'] > 0) | ~df_ls['date'].duplicated()]\n",
        "  #print(df_ls[df_ls.duplicated('date')])\n",
        "  df_ls['duration'] = 0\n",
        "  for co_name in df_ls.columns:\n",
        "    if 'seconds' in str(co_name) and not 'avg' in str(co_name):\n",
        "      df_ls.duration +=df_ls[co_name]\n",
        "  df_ls.to_csv(path.join(preprocessed_d_path,'level_sleep.csv'))\n",
        "  return df_ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNilLq4TKD6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_sleep_sleepscore(d_path,preprocessed_d_path):\n",
        "  if (path.exists(path.join(d_path,'sleep.csv')) ==False or path.exists(path.join(d_path,'sleep_score.csv')) == False or path.exists(path.join(d_path,'level_sleep.csv'))==False):\n",
        "    return\n",
        "  df = pp_sleep(d_path,preprocessed_d_path)\n",
        "\n",
        "  df_ss = pp_sleepscore(d_path,preprocessed_d_path)\n",
        "  \n",
        "  \n",
        "  #str_column_names = 'date,deep_count,deep_minutes,deep_thirty_day_avg_minutes,wake_count,wake_minutes,wake_thirty_day_avg_minutes,light_count,light_minutes,light_thirty_day_avg_minutes,rem_count,rem_minutes,rem_thirty_day_avg_minutes,overall_score,composition_score,revitalization_score,duration_score,deep_sleep_in_minutes,resting_heart_rate,restlessness'\n",
        "  #column_names = str_column_names.split(',')\n",
        "  #column_new_names = str_column_names.replace('minutes','seconds').split(',')\n",
        "\n",
        "  df_result = df.merge(df_ss,how='outer',on='endTime')\n",
        "  for co_name in df_result.columns:\n",
        "    if str(co_name).count('minutes')>0:\n",
        "      df_result.rename(columns={co_name:co_name.replace('minutes','seconds')},inplace=True)\n",
        "  print(df_result)\n",
        "  if df_result.empty:\n",
        "    df_result.to_csv(path.join(preprocessed_d_path,'ssleep_score.csv'))\n",
        "    return df_result\n",
        "  df_result.sort_values(by = 'endTime', ascending=False, inplace=True)\n",
        "  df_result.duration = df_result.duration/1000\n",
        "  df_result.timeInBed = df_result.timeInBed*60\n",
        "  df_result.drop_duplicates(inplace=True)\n",
        "  \n",
        "  #boolean = df_result['endTime'].duplicated().any()\n",
        "  boolean = False\n",
        "  if boolean:\n",
        "    print(\"ssleep score duplicated\")\n",
        "    print(df_result[df_result.duplicated()])\n",
        "    #print(df_result[df_result.duplicated('endTime')])\n",
        "    print('---------------------------------1----------------------------------')\n",
        "  df_result.to_csv(path.join(preprocessed_d_path,'ssleep_score.csv'))\n",
        "  return df_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vmUE20Wpyb3",
        "colab_type": "text"
      },
      "source": [
        "join(level_sleep,ssleep_score) \\\n",
        "-deep_minutes = deep_sleep_in_seconds \\\n",
        "-wake_minutes = secondsAwake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UEyucclCRWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_data_sleep(d_path,preprocessed_d_path):\n",
        "  if (path.exists(path.join(d_path,'sleep.csv')) ==False or path.exists(path.join(d_path,'sleep_score.csv')) == False or path.exists(path.join(d_path,'level_sleep.csv'))==False):\n",
        "    return\n",
        "  df_ls = pp_levelsleep(d_path,preprocessed_d_path)\n",
        "\n",
        "  df_sss = pp_sleep_sleepscore(d_path,preprocessed_d_path)\n",
        "  df_sss.rename(columns={'deep_sleep_in_seconds':'deep_seconds','secondsToFallAsleep':'fall_asleep_seconds','secondsAsleep':'asleep_seconds','endTime':'end_time','secondsAwake':'wake_seconds','secondsAfterWakeup':'after_wake_seconds','timeInBed':'in_bed_seconds','infoCode':'info_code','mainSleep':'main_sleep'},inplace=True)\n",
        "  df_sss.drop(['deep_seconds','wake_seconds'],axis='columns',inplace=True)\n",
        "  \n",
        "  df_result = df_sss.merge(df_ls,how='outer',on=['date','duration'])\n",
        "  if df_result.empty:\n",
        "    df_result.to_csv(path.join(preprocessed_d_path,'sleep_data.csv'))\n",
        "    return\n",
        "  df_result.sort_values(by = 'date',  ascending=False, inplace=True)\n",
        "  print('d_path: {}\\ndf level sleep: {}\\ndf ssleep core: {}\\ndf result: {}\\n'.format(d_path,len(df_ls.index),len(df_sss.index),len(df_result.index)))\n",
        "  df_result.to_csv(path.join(preprocessed_d_path,'sleep_data.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ovKiyipUSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_data_exercise(d_path,preprocessed_d_path):\n",
        "  file_path = path.join(d_path,'exercise.csv')\n",
        "  print(file_path)\n",
        "  if (path.exists(file_path)) ==False:\n",
        "    return\n",
        "  column_name = 'activityName,averageHeartRate,calories,distance,duration,steps,speed,startTime,elevationGain,sedentary_active_minutes,lightly_active_minutes,fairly_active_minutes,very_active_minutes,time_in_fat_burn_heart_rate_zone,time_in_out_of_range_heart_rate_zone,time_in_peak_heart_rate_zone,time_in_cardio_heart_rate_zone,vo2Max'.split(',')\n",
        "  df = pd.read_csv(file_path)[column_name]\n",
        "  for co_name in df.columns:\n",
        "    if 'minutes' in co_name:\n",
        "      df[co_name] = df[co_name]*60\n",
        "      df.rename(columns={co_name:co_name.replace('minutes','seconds')},inplace=True)\n",
        "  df.rename(columns={'activityName':'activity_name','averageHeartRate':'average_heart_rate','startTime':'start_time','elevationGain':'elevantion_gain'},inplace=True)\n",
        "  df.duration = df.duration/1000\n",
        "  #df.start_time = pd.to_datetime(df.start_time)\n",
        "  #df.start_time = df.start_time.dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "  df.start_time = pd.to_datetime(df.start_time).values.astype(np.int64) // 10 ** 9\n",
        "  df.sort_values(by='start_time', ascending = False, inplace=True)\n",
        "  new_column_name = 'injuries,fatigue,mood,readiness,calories_from_file,distance_from_file,steps_from_file,avg_heart_rate_from_file'.replace(' ','').replace('\\n','').split(',')\n",
        "  for co_name in new_column_name:\n",
        "    df[co_name] = 0\n",
        "  df['injuries'] = '{'\n",
        "  \n",
        "  file_path = path.join(d_path,'total_cal_dis_ste_hea.csv')\n",
        "  df_tot = pd.read_csv(file_path)\n",
        "  df_tot.dateTime = pd.to_datetime(df_tot.dateTime).values.astype(np.int64) // 10 ** 9\n",
        "  df_tot.calories[pd.isna(df_tot.calories)] = 0\n",
        "  df_tot.distance[pd.isna(df_tot.distance)] = 0\n",
        "  df_tot.steps[pd.isna(df_tot.steps)] = 0\n",
        "  df_tot.heart_rate[pd.isna(df_tot.heart_rate)] = 0\n",
        "  df_tot.sort_values(by = 'dateTime',ascending=True, inplace=True)\n",
        "\n",
        "  df_wel = []\n",
        "  file_path = path.join(d_path,'wellness.csv')\n",
        "  if (path.exists(file_path)):\n",
        "    df_wel = pd.read_csv(file_path)\n",
        "    df_wel.effective_time_frame = df_wel.effective_time_frame.str.replace('T',' ').str.split('.').str[0]\n",
        "    df_wel.effective_time_frame = pd.to_datetime(df_wel.effective_time_frame).values.astype(np.int64) // 10 ** 9\n",
        "    df_wel.sort_values(by = 'effective_time_frame',ascending=True, inplace=True)\n",
        "\n",
        "  df_in = []\n",
        "  file_path = path.join(d_path,'injury.csv')\n",
        "  if (path.exists(file_path)):\n",
        "    df_in = pd.read_csv(file_path)\n",
        "    df_in.effective_time_frame = df_in.effective_time_frame.str.replace('T',' ').str.split('.').str[0]\n",
        "    df_in.effective_time_frame = pd.to_datetime(df_in.effective_time_frame).values.astype(np.int64) // 10 ** 9\n",
        "    df_in.sort_values(by = 'effective_time_frame',ascending=True, inplace=True)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    count = 0\n",
        "    # for inde in range(len(df_tot)):\n",
        "    #   #print(df_tot.iloc[inde])\n",
        "    #   if df.at[i,'start_time'] + df.at[i,'duration'] < df_tot.at[inde,'dateTime']:\n",
        "    #     break\n",
        "    #   if df.at[i,'start_time'] <= df_tot.at[inde,'dateTime']:\n",
        "    #     count +=1\n",
        "    #     df.at[i,'calories_from_file'] += df_tot.at[inde,'calories']\n",
        "    #     df.at[i,'distance_from_file'] += df_tot.at[inde,'distance']\n",
        "    #     df.at[i,'steps_from_file'] += df_tot.at[inde,'steps']\n",
        "    #     df.at[i,'avg_heart_rate_from_file'] += df_tot.at[inde,'heart_rate']\n",
        "    # if count != 0:\n",
        "    #   df.at[i,'avg_heart_rate_from_file'] = df.at[i,'avg_heart_rate_from_file'] / count\n",
        "    \n",
        "    # for inde in range(len(df_wel)):\n",
        "    #   if df.at[i,'start_time'] + df.at[i,'duration'] < df_wel.at[inde,'effective_time_frame']:\n",
        "    #     break\n",
        "    #   if df.at[i,'start_time'] <= df_wel.at[inde,'effective_time_frame']:\n",
        "    #     df.at[i,'fatigue'] += df_wel.at[inde,'fatigue']\n",
        "    #     df.at[i,'mood'] += df_wel.at[inde,'mood']\n",
        "    #     df.at[i,'readiness'] += df_wel.at[inde,'readiness']\n",
        "\n",
        "    # for inde in range(len(df_in)):\n",
        "    #   if df.at[i,'start_time'] + df.at[i,'duration'] < df_in.at[inde,'effective_time_frame']:\n",
        "    #     break\n",
        "    #   if df.at[i,'start_time'] <= df_in.at[inde,'effective_time_frame']:\n",
        "    #     df.at[i,'injuries'] += df_in.at[inde,'injuries'].replace('{','').replace('}','') +'|'\n",
        "\n",
        "    st = df.at[i,'start_time']\n",
        "    en = df.at[i,'start_time'] + df.at[i,'duration']\n",
        "    pst = 10**15\n",
        "    #df_tot\n",
        "    if (len(df_tot)>0):\n",
        "      ar_tot = df_tot['dateTime'].to_numpy()\n",
        "      pst = np.searchsorted(ar_tot, st)\n",
        "    #print(ar_tot)\n",
        "    #print(pst)\n",
        "    for inde in range(pst,len(df_tot)):\n",
        "      if en < df_tot.at[inde,'dateTime']:\n",
        "        break\n",
        "      count +=1\n",
        "      df.at[i,'calories_from_file'] += df_tot.at[inde,'calories']\n",
        "      df.at[i,'distance_from_file'] += df_tot.at[inde,'distance']\n",
        "      df.at[i,'steps_from_file'] += df_tot.at[inde,'steps']\n",
        "      df.at[i,'avg_heart_rate_from_file'] += df_tot.at[inde,'heart_rate']\n",
        "    if count != 0:\n",
        "      df.at[i,'avg_heart_rate_from_file'] = df.at[i,'avg_heart_rate_from_file'] / count\n",
        "    \n",
        "    pst = 10**15\n",
        "    #df_wel\n",
        "    if (len(df_wel)>0):\n",
        "      ar_wel = df_wel['effective_time_frame'].to_numpy()\n",
        "      pst = np.searchsorted(ar_wel,st)\n",
        "    for inde in range(pst,len(df_wel)):\n",
        "      if en < df_wel.at[inde,'effective_time_frame']:\n",
        "        break\n",
        "      df.at[i,'fatigue'] += df_wel.at[inde,'fatigue']\n",
        "      df.at[i,'mood'] += df_wel.at[inde,'mood']\n",
        "      df.at[i,'readiness'] += df_wel.at[inde,'readiness']\n",
        "    \n",
        "    pst = 10**15\n",
        "    #df_in\n",
        "    if (len(df_in)>0):\n",
        "      ar_in = df_in['effective_time_frame'].to_numpy()\n",
        "      pst = np.searchsorted(ar_in,st)\n",
        "    for inde in range(pst,len(df_in)):\n",
        "      if en < df_in.at[inde,'effective_time_frame']:\n",
        "        break\n",
        "      df.at[i,'injuries'] += df_in.at[inde,'injuries'].replace('{','').replace('}','') +'|'\n",
        "\n",
        "  df.injuries +='}'\n",
        "  df.to_csv(path.join(preprocessed_d_path,'exercise_data.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdDUhDdl-igF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_data_reporting(d_path,preprocessed_d_path):\n",
        "  file_path = path.join(d_path,'reporting.csv')\n",
        "  print(file_path)\n",
        "  if (path.exists(file_path)) ==False:\n",
        "    return\n",
        "  df = pd.read_csv(file_path)\n",
        "  df.drop(['Timestamp','unused'],axis='columns',inplace=True)\n",
        "  df.rename(columns={'Whatdayyoureportfor?':'date','Eatenmeals':'meal','Whatisyourweight?':'weight','\"Howmanyglassesoffluiddidyoudrink(water|milk|juice...)\"':'glasses','Didyoudrinkalcohol?':'alcohol'},inplace=True)\n",
        "  df.date = pd.to_datetime(df.date,format='%d/%m/%Y')\n",
        "  df.date = pd.to_datetime(df.date,format='%Y-%m-%d')\n",
        "  df.sort_values(by = 'date',ascending=False, inplace= True)\n",
        "  new_column_names = 'very_active,moderately_active,lightly_active,sedentary,calories,distance,steps,heart_rate'.split(',')\n",
        "  for co_name in new_column_names:\n",
        "    df[co_name] = 0\n",
        "\n",
        "  file_path = path.join(d_path,'total_cal_dis_ste_hea.csv')\n",
        "  df_tot = pd.read_csv(file_path)\n",
        "  df_tot.rename(columns={'dateTime':'date'},inplace=True)\n",
        "  df_tot.date = df_tot.date.str.replace('\\n','').str.split(' ').str[0]\n",
        "  df_tot.date = pd.to_datetime(df_tot.date,format='%m/%d/%y')\n",
        "  df_tot.date = pd.to_datetime(df_tot.date,format='%Y-%m-%d')\n",
        "  #df_tot['count'] = 1\n",
        "  df_tot.insert(0,'count',int(1))\n",
        "  df_tot.count = df_tot['count'].astype('int64')\n",
        "  df_tot = df_tot.groupby(['date']).sum()\n",
        "  df_tot.count = df_tot['count'].astype('int64')\n",
        "  #print(type(df_tot.count[0]))\n",
        "  #print(pd.isna(df_tot.heart_rate).index)\n",
        "  #df_tot.loc['heart_rate',pd.isna(df_tot.heart_rate).index] = 0\n",
        "  df_tot.heart_rate[pd.isna(df_tot.heart_rate)] = 0\n",
        "  #df_tot.heart_rate = df_tot.heart_rate/df_tot.count\n",
        "  #print(type(df_tot.heart_rate[0]))\n",
        "  df_tot.heart_rate = df_tot.heart_rate.div(df_tot.count,axis=0)\n",
        "  df_tot.drop('count',axis='columns',inplace=True)\n",
        "  df_tot.calories[pd.isna(df_tot.calories)] = 0\n",
        "  df_tot.distance[pd.isna(df_tot.distance)] = 0\n",
        "  df_tot.steps[pd.isna(df_tot.steps)] = 0\n",
        "  df_tot.heart_rate[pd.isna(df_tot.heart_rate)] = 0\n",
        "  # df_tot.loc['calories',pd.isna(df_tot.calories).index] = 0\n",
        "  # df_tot.loc['distance',pd.isna(df_tot.distance).index] = 0\n",
        "  # df_tot.loc['steps',pd.isna(df_tot.steps).index] = 0\n",
        "  # df_tot.loc['heart_rate',pd.isna(df_tot.heart_rate).index] = 0\n",
        "  \n",
        "\n",
        "  file_path = path.join(d_path,'total_active_minutes.csv')\n",
        "  df_tm = pd.read_csv(file_path)\n",
        "  df_tm.rename(columns={'dateTime':'date'},inplace=True)\n",
        "  df_tm.date = df_tm.date.str.split(' ').str[0]\n",
        "  df_tm.date = pd.to_datetime(df_tm.date,format='%m/%d/%y')\n",
        "  df_tm.date = pd.to_datetime(df_tm.date,format='%Y-%m-%d')\n",
        "  for co_name in 'very_active,moderately_active,lightly_active,sedentary'.split(','):\n",
        "    df_tm[co_name] = df_tm[co_name]*60\n",
        "  df_tm = df_tm.groupby(['date']).sum()\n",
        "  #print(df_tm.index)\n",
        "\n",
        "  for co_name in new_column_names:\n",
        "    #print(co_name)\n",
        "    if co_name in df_tot.columns:\n",
        "      for i in range(len(df)):\n",
        "        if df.at[i,'date'] in df_tot.index:\n",
        "          #print('df_tot')\n",
        "          df.at[i,co_name] = df_tot.at[df.at[i,'date'],co_name]\n",
        "      # if co_name == 'heart_rate':\n",
        "      #   df[co_name] = df[co_name]/len(df_tot[co_name])\n",
        "    if co_name in df_tm.columns:\n",
        "      for i in range(len(df)):\n",
        "        if df.at[i,'date'] in df_tm.index:\n",
        "          #print('df_tm')\n",
        "          df.at[i,co_name] = df_tm.at[df.at[i,'date'],co_name]\n",
        "  \n",
        "  df.to_csv(path.join(preprocessed_d_path,'report_data.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rDiG8g8Njl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_merged_data():\n",
        "  for d in listdir(MERGED_DIR):\n",
        "    d_path = path.join(MERGED_DIR,d)\n",
        "    if path.isfile(d_path):\n",
        "      continue\n",
        "    \n",
        "    preprocessed_d_path = path.join(PRE_PROCESSED_DIR,d)\n",
        "    if not(path.exists(preprocessed_d_path)):\n",
        "      mkdir(preprocessed_d_path)\n",
        "     \n",
        "    #pp_data_sleep(d_path,preprocessed_d_path)\n",
        "    #pp_data_exercise(d_path,preprocessed_d_path)\n",
        "    #pp_data_reporting(d_path,preprocessed_d_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMBYytnDGBgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  #convert_fitbit_folder()\n",
        "  #merge_fitbit_folder()\n",
        "  #merge_multi_file_type_of_fitbit_folder(MERGED_DIR, MERGED_DIR)\n",
        "  #convert_pmsys_folder()\n",
        "  #convert_googledocs_folder()\n",
        "  #copy_csv_merge_folder()\n",
        "  preprocess_merged_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTsVDZaKMlTb",
        "colab_type": "code",
        "outputId": "8c100a3e-2b48-4ab9-9b0d-5b1756d7190c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "main()\n",
        "# if __name__ == 'main':\n",
        "#   main()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./spll/data/merged/p09/exercise.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "./spll/data/merged/p09/reporting.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "./spll/data/merged/p04/exercise.csv\n",
            "./spll/data/merged/p04/reporting.csv\n",
            "./spll/data/merged/p03/exercise.csv\n",
            "./spll/data/merged/p03/reporting.csv\n",
            "./spll/data/merged/p12/exercise.csv\n",
            "./spll/data/merged/p12/reporting.csv\n",
            "./spll/data/merged/p11/exercise.csv\n",
            "./spll/data/merged/p11/reporting.csv\n",
            "./spll/data/merged/p07/exercise.csv\n",
            "./spll/data/merged/p07/reporting.csv\n",
            "./spll/data/merged/p02/exercise.csv\n",
            "./spll/data/merged/p02/reporting.csv\n",
            "./spll/data/merged/p06/exercise.csv\n",
            "./spll/data/merged/p06/reporting.csv\n",
            "./spll/data/merged/p10/exercise.csv\n",
            "./spll/data/merged/p10/reporting.csv\n",
            "./spll/data/merged/p05/exercise.csv\n",
            "./spll/data/merged/p05/reporting.csv\n",
            "./spll/data/merged/p01/exercise.csv\n",
            "./spll/data/merged/p01/reporting.csv\n",
            "./spll/data/merged/p08/exercise.csv\n",
            "./spll/data/merged/p08/reporting.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZWak2tcElYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!zip -r /content/converted.zip /content/spll/data/converted\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/converted.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUIPQCRpV3tU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!zip -r /content/merged.zip /content/spll/data/merged\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/merged.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa8Fa98vgVaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2634bf9-952d-4d18-dd3a-d10fdad99346"
      },
      "source": [
        "!zip -r /content/preprocessed.zip /content/spll/data/preprocessed\n",
        "from google.colab import files\n",
        "files.download(\"/content/preprocessed.zip\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/spll/data/preprocessed/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p09/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p09/level_sleep.csv (deflated 71%)\n",
            "  adding: content/spll/data/preprocessed/p09/sleep_data.csv (deflated 70%)\n",
            "  adding: content/spll/data/preprocessed/p09/report_data.csv (deflated 64%)\n",
            "  adding: content/spll/data/preprocessed/p09/exercise_data.csv (deflated 64%)\n",
            "  adding: content/spll/data/preprocessed/p09/ssleep_score.csv (deflated 67%)\n",
            "  adding: content/spll/data/preprocessed/p04/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p04/report_data.csv (deflated 69%)\n",
            "  adding: content/spll/data/preprocessed/p04/exercise_data.csv (deflated 65%)\n",
            "  adding: content/spll/data/preprocessed/p03/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p03/report_data.csv (deflated 71%)\n",
            "  adding: content/spll/data/preprocessed/p03/exercise_data.csv (deflated 66%)\n",
            "  adding: content/spll/data/preprocessed/p12/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p12/report_data.csv (deflated 71%)\n",
            "  adding: content/spll/data/preprocessed/p12/exercise_data.csv (deflated 66%)\n",
            "  adding: content/spll/data/preprocessed/p11/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p11/report_data.csv (deflated 70%)\n",
            "  adding: content/spll/data/preprocessed/p11/exercise_data.csv (deflated 65%)\n",
            "  adding: content/spll/data/preprocessed/p07/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p07/report_data.csv (deflated 68%)\n",
            "  adding: content/spll/data/preprocessed/p07/exercise_data.csv (deflated 63%)\n",
            "  adding: content/spll/data/preprocessed/p02/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p02/report_data.csv (deflated 78%)\n",
            "  adding: content/spll/data/preprocessed/p02/exercise_data.csv (deflated 68%)\n",
            "  adding: content/spll/data/preprocessed/p06/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p06/report_data.csv (deflated 76%)\n",
            "  adding: content/spll/data/preprocessed/p06/exercise_data.csv (deflated 65%)\n",
            "  adding: content/spll/data/preprocessed/p10/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p10/report_data.csv (deflated 72%)\n",
            "  adding: content/spll/data/preprocessed/p10/exercise_data.csv (deflated 68%)\n",
            "  adding: content/spll/data/preprocessed/p05/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p05/report_data.csv (deflated 69%)\n",
            "  adding: content/spll/data/preprocessed/p05/exercise_data.csv (deflated 70%)\n",
            "  adding: content/spll/data/preprocessed/p01/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p01/report_data.csv (deflated 70%)\n",
            "  adding: content/spll/data/preprocessed/p01/exercise_data.csv (deflated 66%)\n",
            "  adding: content/spll/data/preprocessed/p08/ (stored 0%)\n",
            "  adding: content/spll/data/preprocessed/p08/report_data.csv (deflated 68%)\n",
            "  adding: content/spll/data/preprocessed/p08/exercise_data.csv (deflated 65%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-a56c0db3b7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r /content/preprocessed.zip /content/spll/data/preprocessed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/preprocessed.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    }
  ]
}